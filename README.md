# BERT from Scratch ðŸ§ ðŸ“š

This repository implements **BERT (Bidirectional Encoder Representations from Transformers)** from scratch using PyTorch. The purpose is to build a deeper understanding of the BERT architecture including self-attention, transformer blocks, positional encoding, and masked language modeling.

---

## ðŸš€ What is BERT?

BERT is a transformer-based model introduced by Google in 2018. It reads input text bidirectionally and captures rich context from both left and right sides of a token, setting new benchmarks in multiple NLP tasks.

> ðŸ“„ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

---

## ðŸ§  Key Features

- âœ… Custom Tokenizer (WordPiece-inspired)
- âœ… Positional Encoding
- âœ… Multi-Head Self-Attention
- âœ… Transformer Encoder Blocks
- âœ… Masked Language Modeling (MLM)
- âœ… Layer Normalization + Residual Connections
- âœ… Training loop with toy data

---
